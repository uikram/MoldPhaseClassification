{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.Preprocessing import filter,features,plots\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Organized Data/china/abb_china_36501_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'HOUR' is your time column\n",
    "df['HOUR'] = pd.to_datetime(df['HOUR'])\n",
    "\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "features = df[features_considered]\n",
    "\n",
    "# Standardizing the features\n",
    "dataset = features.values\n",
    "data_mean = dataset.mean(axis=0)\n",
    "data_std = dataset.std(axis=0)\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 30\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "X_train, y_train = create_dataset(df[features_considered], df['PHASE'], time_steps)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'HOUR' is your time column\n",
    "df['HOUR'] = pd.to_datetime(df['HOUR'])\n",
    "df = df.sort_values('HOUR')\n",
    "\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v[..., np.newaxis])  # Adding an extra dimension for each feature\n",
    "    return np.array(Xs)\n",
    "\n",
    "time_steps = 30\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "for feature in features_considered:\n",
    "    X_train = create_dataset(df[feature], time_steps)\n",
    "    print(f\"{feature} shape: {X_train.shape}\")\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'HOUR' is your time column\n",
    "df['HOUR'] = pd.to_datetime(df['HOUR'])\n",
    "df = df.sort_values('HOUR')\n",
    "\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)\n",
    "\n",
    "time_steps = 30\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "X_train = create_dataset(df[features_considered], time_steps)\n",
    "print(f\"Tensor shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame and 'HOUR' is your time column\n",
    "df['HOUR'] = pd.to_datetime(df['HOUR'])\n",
    "df = df.sort_values('HOUR')\n",
    "\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "\n",
    "# One-hot encoding the 'PHASE' feature\n",
    "y = pd.get_dummies(df['PHASE']).values\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 720  # Represents 30 days of hourly data\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "X, y = create_dataset(df[features_considered], y, time_steps)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12664, 720, 3), (12664, 720, 4)\n",
      "Testing set shape: (3167, 720, 3), (3167, 720, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame and 'HOUR' is your time column\n",
    "df['HOUR'] = pd.to_datetime(df['HOUR'])\n",
    "df = df.sort_values('HOUR')\n",
    "\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "\n",
    "# One-hot encoding the 'PHASE' feature\n",
    "y = pd.get_dummies(df['PHASE']).values\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i:i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "time_steps = 720  # Represents 30 days of hourly data\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "X, y = create_dataset(df[features_considered], y, time_steps)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the desired proportion for each phase in the testing set\n",
    "# # For example, if you want 25% of each phase in the testing set, you can set it to 0.25\n",
    "# testing_proportion = 0.25\n",
    "\n",
    "# # Create a dictionary to keep track of how many samples from each phase are included in the testing set\n",
    "# phase_count = {phase: int(len(y) * testing_proportion) for phase in range(y.shape[1])}\n",
    "\n",
    "# # Lists to store indices for the training and testing sets\n",
    "# train_indices, test_indices = [], []\n",
    "\n",
    "# # Shuffle the data while ensuring that each phase is represented in both sets\n",
    "# for i in range(len(X)):\n",
    "#     phase = np.argmax(y[i])\n",
    "#     if phase_count[phase] > 0:\n",
    "#         test_indices.append(i)\n",
    "#         phase_count[phase] -= 1\n",
    "#     else:\n",
    "#         train_indices.append(i)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train = X[train_indices]\n",
    "# X_test = X[test_indices]\n",
    "# y_train = y[train_indices]\n",
    "# y_test = y[test_indices]\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "396/396 - 74s - loss: 1.2160 - accuracy: 0.4640\n",
      "Epoch 2/10\n",
      "396/396 - 68s - loss: 1.1077 - accuracy: 0.5082\n",
      "Epoch 3/10\n",
      "396/396 - 77s - loss: 1.0383 - accuracy: 0.5443\n",
      "Epoch 4/10\n",
      "396/396 - 80s - loss: 0.9725 - accuracy: 0.5720\n",
      "Epoch 5/10\n",
      "396/396 - 68s - loss: 0.9182 - accuracy: 0.5915\n",
      "Epoch 6/10\n",
      "396/396 - 71s - loss: 0.8735 - accuracy: 0.6097\n",
      "Epoch 7/10\n",
      "396/396 - 67s - loss: 0.8454 - accuracy: 0.6302\n",
      "Epoch 8/10\n",
      "396/396 - 71s - loss: 0.7673 - accuracy: 0.6733\n",
      "Epoch 9/10\n",
      "396/396 - 73s - loss: 0.7195 - accuracy: 0.6950\n",
      "Epoch 10/10\n",
      "396/396 - 68s - loss: 0.6825 - accuracy: 0.7178\n",
      "99/99 - 5s - loss: 0.6464 - accuracy: 0.7368\n",
      "Loss: 0.6463587880134583, Accuracy: 0.7368022799491882\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are your training and testing sets\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[2]\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(TimeDistributed(Dense(n_outputs, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# Function to fine-tune hyperparameters\n",
    "def fine_tune_model(X_train, y_train):\n",
    "    model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "    # define the grid search parameters\n",
    "    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    param_grid = dict(optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Create the LSTM model\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Fine-tune hyperparameters\n",
    "# fine_tune_model(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('abb_china_36501.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 - 28s - loss: 3.1848 - accuracy: 0.2599\n",
      "Loss: 3.184824228286743, Accuracy: 0.2598792016506195\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Organized Data/germany/abb_germany_225414_statistics.csv')\n",
    "# Selecting features\n",
    "features_considered = ['CT', 'TAV', 'SHOT_COUNT']\n",
    "\n",
    "# One-hot encoding the 'PHASE' feature\n",
    "y = pd.get_dummies(df['PHASE']).values\n",
    "\n",
    "# Creating the sliding window\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i:i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "time_steps = 720  # Represents 30 days of hourly data\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "X, y = create_dataset(df[features_considered], y, time_steps)\n",
    "\n",
    "loss, accuracy = model.evaluate(X, y, verbose=2)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0., 273., 105.],\n",
       "        [  0., 274.,   0.],\n",
       "        [  0., 275.,   0.],\n",
       "        ...,\n",
       "        [146., 363., 246.],\n",
       "        [146., 362., 246.],\n",
       "        [147., 361., 246.]],\n",
       "\n",
       "       [[  0., 274.,   0.],\n",
       "        [  0., 275.,   0.],\n",
       "        [  0., 276.,   0.],\n",
       "        ...,\n",
       "        [146., 362., 246.],\n",
       "        [147., 361., 246.],\n",
       "        [146., 362., 246.]],\n",
       "\n",
       "       [[  0., 275.,   0.],\n",
       "        [  0., 276.,   0.],\n",
       "        [  0., 274.,   0.],\n",
       "        ...,\n",
       "        [147., 361., 246.],\n",
       "        [146., 362., 246.],\n",
       "        [146., 363., 245.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0., 290.,   0.],\n",
       "        [  0., 284.,   0.],\n",
       "        [  0., 282.,   0.],\n",
       "        ...,\n",
       "        [  0., 299.,   0.],\n",
       "        [  0., 296.,   0.],\n",
       "        [  0., 296.,   0.]],\n",
       "\n",
       "       [[  0., 284.,   0.],\n",
       "        [  0., 282.,   0.],\n",
       "        [  0., 283.,   0.],\n",
       "        ...,\n",
       "        [  0., 296.,   0.],\n",
       "        [  0., 296.,   0.],\n",
       "        [  0., 292.,   0.]],\n",
       "\n",
       "       [[  0., 282.,   0.],\n",
       "        [  0., 283.,   0.],\n",
       "        [  0., 285.,   0.],\n",
       "        ...,\n",
       "        [  0., 296.,   0.],\n",
       "        [  0., 292.,   0.],\n",
       "        [  0., 291.,   0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=filter(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tav_arr,sc_arr,ct_arr=features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
